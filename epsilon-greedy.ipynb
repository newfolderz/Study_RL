{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4c48f4",
   "metadata": {},
   "source": [
    "Link: [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/multi-armed-bandit-problem-in-reinforcement-learning/)\n",
    "\n",
    "[github](https://gibberblot.github.io/rl-notes/single-agent/multi-armed-bandits.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53584d3c",
   "metadata": {},
   "source": [
    "**Bài toán**: \n",
    "- Có $N$ máy đánh bạc một tay (loại máy có \"cánh tay\" bên cạnh để người chơi kéo để máy chạy lại)\n",
    "- Sau mỗi lần kéo máy sẽ trả lại một phần thưởng từ một phân phối xác suất chưa xác định \n",
    "- Một số máy có phần thưởng cao hơn các máy khác (nhưng ta không biết)\n",
    "- Số lần kéo thường là hữu hạn\n",
    "\n",
    "**Mục tiêu**: Tối đa hóa phần thưởng nhận được "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e6381",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy: \n",
    "- Mô phỏng bài toán Multi-Armed Bandit \n",
    "- Triển khai thuật toán Epsilon-Greedy: \n",
    "<br> Với xác suất $\\epsilon$ ($\\epsilon$ nhỏ), chọn một cánh tay ngẫu nhiên \n",
    "<br> Ngược lại, chọn hành động tham lam \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7342a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4242eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    ''' \n",
    "    Đại diện cho một policy: Chọn một hành động khám phá với xác suất nhỏ, \n",
    "    còn lại là tham lam \n",
    "    '''\n",
    "    def __init__(self, n_arms, epsilon):\n",
    "        ''' \n",
    "        Hàm dựng: \n",
    "        - n_arms: số cánh tay có thể chọn\n",
    "        - epsilon: xác suất chọn hành động khám phá\n",
    "        - counts([]): mảng đếm, để tính values \n",
    "        - values([])\n",
    "        '''\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "    \n",
    "    def select_arm(self):\n",
    "        '''\n",
    "        Xác suất explore: epsion, nên xs chọn một hành động trong TH này là epsilon/n => lấy pp đều \n",
    "        Còn lại: chọn hành động tham lam \n",
    "        '''\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_arms)\n",
    "        else:\n",
    "            return np.argmax(self.values)\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        ''' \n",
    "        Với arm được chọn: V[t] = (n-1)/n * V[t-1] + 1/n * R\n",
    "        Các arm khác không thay đổi \n",
    "        '''\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value_past = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = (n-1) / n * value_past + 1/n * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cedd530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual mean rewards for each arm: \n",
      " [ 1.    0.25 -1.84  2.44  1.17 -0.36 -0.44 -0.47  1.01  2.21]\n",
      "The best arm is #3 with a mean of 2.44 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Mẫu chạy thử '''\n",
    "n_arms = 10\n",
    "epsilon = 0.1 \n",
    "T = 1000\n",
    "\n",
    "''' \n",
    "Thiết lập trạng thái ban đầu \n",
    "true_means : kỳ vọng reward của một máy \n",
    "'''\n",
    "true_means = np.random.randn(n_arms)\n",
    "print(f\"Actual mean rewards for each arm: \\n {np.round(true_means, 2)}\")\n",
    "print(f\"The best arm is #{np.argmax(true_means)} with a mean of {np.max(true_means):.2f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17c3906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: [ 0.81 -0.04 -2.35  2.12  1.18 -0.13 -0.64 -0.16  0.63  2.24] \n",
      "\n",
      "Total Reward: 2080.33\n",
      "Var:  0.0906\n"
     ]
    }
   ],
   "source": [
    "# Test voi T = 1000\n",
    "''' reward: giả định tuân theo phân phối chuẩn, kỳ vọng = true_mean '''\n",
    "agent = EpsilonGreedy(n_arms, epsilon)\n",
    "total_reward = 0\n",
    "\n",
    "for t in range(T):\n",
    "    arm_to_pull = agent.select_arm()\n",
    "    reward = np.random.randn() + true_means[arm_to_pull]\n",
    "    agent.update(arm_to_pull, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Values: {np.round(agent.values, 2)} \\n\")\n",
    "print(f\"Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "Var = sum((agent.values-true_means)**2) / (n_arms -1)\n",
    "print(f\"Var: \", round(Var,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52979024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: [ 1.04  0.06 -1.87  2.44  1.27 -0.44 -0.55 -0.52  1.08  2.18] \n",
      "\n",
      "Total Reward: 22165.04\n",
      "Var:  0.0085\n"
     ]
    }
   ],
   "source": [
    "# Test voi T = 10000\n",
    "agent = EpsilonGreedy(n_arms, epsilon)\n",
    "total_reward = 0\n",
    "\n",
    "for t in range(10000):\n",
    "    arm_to_pull = agent.select_arm()\n",
    "    reward = np.random.randn() + true_means[arm_to_pull]\n",
    "    agent.update(arm_to_pull, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Values: {np.round(agent.values, 2)} \\n\")\n",
    "print(f\"Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "Var = sum((agent.values-true_means)**2) / (n_arms -1)\n",
    "print(f\"Var: \", round(Var,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "514cf408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: [ 1.01  0.24 -1.84  2.44  1.11 -0.37 -0.46 -0.48  1.04  2.18] \n",
      "\n",
      "Total Reward: 224287.53\n",
      "Var:  0.0006\n"
     ]
    }
   ],
   "source": [
    "# Test voi T = 100000 \n",
    "agent = EpsilonGreedy(n_arms, epsilon)\n",
    "total_reward = 0\n",
    "\n",
    "for t in range(100000):\n",
    "    arm_to_pull = agent.select_arm()\n",
    "    reward = np.random.randn() + true_means[arm_to_pull]\n",
    "    agent.update(arm_to_pull, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Values: {np.round(agent.values, 2)} \\n\")\n",
    "print(f\"Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "Var = sum((agent.values-true_means)**2) / (n_arms -1)\n",
    "print(f\"Var: \", round(Var,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

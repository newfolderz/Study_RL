{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3644af0d",
   "metadata": {},
   "source": [
    "Link: [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/multi-armed-bandit-problem-in-reinforcement-learning/)\n",
    "\n",
    "[github](https://gibberblot.github.io/rl-notes/single-agent/multi-armed-bandits.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f643311",
   "metadata": {},
   "source": [
    "### Bài toán \n",
    "Động lực: không thể chọn explore và exploit cùng lúc \n",
    "\n",
    "Giải pháp: lựa chọn Hành động theo giới hạn trên khoảng tin cậy (Upper Confidence Bound - UCB)\n",
    "$$ Confident\\_Interval = \\argmax_a \\Big(Q_t(a) + c \\sqrt{\\frac{\\ln(t)}{N_t(a)}} \\Big) $$\n",
    "Khoảng tin cậy cho ta biết độ tin cậy giá trị hành động thực tế của hành động A nằm ở đâu trong vùng này\n",
    "- Nếu vùng này nhỏ: chắc chắc giá trị của A gần với ước tính \n",
    "- Ngược lại, không hcawcs chắn giá trị hành động A gần với ước tính \n",
    "\n",
    "Nguyên tắc: lạc quan với sự không chắc chắn, tức là nếu không biết chọn hành động nào thì chọn hành động có Upper Bound lớn nhất. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bdd7fc",
   "metadata": {},
   "source": [
    "### Cơ sở toán học: \n",
    "(Gemini)\n",
    "\n",
    "- Bất đẳng thức Hoeffding: nôm na là cho thấy xác suất để giá trị thực $\\mu$ vượt quá giá trị trung bình quan sát $\\bar{X}$ một khoảng $\\Delta$ sẽ giảm rất nhanh theo hàm mũ khi mẫu $n$ tăng lên: \n",
    "$$S_n = \\sum_{i=1}^{n} X_i$$\n",
    "$$ P(S_n - E[S_n] \\geq t) \\leq \\exp \\left( -\\frac{2t^2}{\\sum (b_i - a_i)^2} \\right) (1) $$\n",
    "$$ P(\\mu - \\bar{X} > \\Delta) \\le \\exp(-2n\\Delta^2) (2)$$\n",
    "<br> $\\bar{X} \\to \\frac{S_n}{n}$ : trung bình mẫu của $n$ bnn độc lập\n",
    "<br> $\\mu \\to E[\\bar{X}]/n$ : giá trị kỳ vọng thực \n",
    "<br> $\\Delta \\to t/n$ : Độ lệch \n",
    "\n",
    "Mặc định $X_i \\in [0,1]$ nên $(b_i-a_i)^2 = 1$ nên $\\sum_{i=1}^{n} (b_i-a_i)^2 = n$\n",
    "\n",
    "Thay hết mấy cái trên vào (1), đồng thời xoay chiều để tìm \"trần\", ta suy ra 2\n",
    "\n",
    "Tìm upper bound: \n",
    "$$ P(\\mu > \\bar{X} + \\Delta) \\le \\exp(-2n\\Delta^2) = p$$\n",
    "Lấy loganepe 2 vế: \n",
    "$$ -2n \\Delta^2 = ln(p) \\implies \\Delta = \\sqrt{\\frac{\\ln(1/p)}{2n}} $$\n",
    "Làm sao để xác suất rủi ro giảm dần theo $t$ ? Chọn $p = \\frac{1}{t^{\\alpha}}$. Vậy \n",
    "$$\\Delta = \\sqrt{\\frac{\\alpha \\ln(t)}{2n}} = c \\sqrt{\\frac{\\ln(t)}{N_t(a)}}$$\n",
    "\n",
    "- Khoảng tin cậy: $Upper\\_Bound = \\bar{X_a} + U_a$\n",
    "\n",
    "Giá trị được chọn phổ biến là: \n",
    "$$A_t = \\arg \\max_{a} \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$$\n",
    "\n",
    "- Sự hội tụ và luật số lớn: Ý tưởng là khi kéo một tay máy càng nhiều lần, độ tin cậy sẽ nhỏ lại \n",
    "- Lý thuyết Regret (Hối tiếc): Mức độ \"sai lầm\" của thuật toán tuân theo hàm log, tăng chậm theo thời gian, giúp tối ưu về mặt toán học (ít nhất là hơn epsilon-greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574899e9",
   "metadata": {},
   "source": [
    "### Triển khai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61975de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be508abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.total_counts = 0\n",
    "    \n",
    "    def select_arm(self):\n",
    "        ucb_values = self.values + np.sqrt(2 * np.log(self.total_counts+1) / (self.counts + 1e-5))\n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        self.total_counts += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value_past = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = (n-1) / n * value_past + 1/n * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbeb4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual mean rewards for each arm: \n",
      " [-0.47 -0.94  0.8   2.03  0.45 -0.07 -0.35  1.59  0.51 -1.23]\n",
      "The best arm is #3 with a mean of 2.03 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Mẫu chạy thử '''\n",
    "n_arms = 10\n",
    "T = 1000\n",
    "\n",
    "''' \n",
    "Thiết lập trạng thái ban đầu \n",
    "true_means : kỳ vọng reward của một máy \n",
    "'''\n",
    "true_means = np.random.randn(n_arms)\n",
    "print(f\"Actual mean rewards for each arm: \\n {np.round(true_means, 2)}\")\n",
    "print(f\"The best arm is #{np.argmax(true_means)} with a mean of {np.max(true_means):.2f} \\n\")\n",
    "agent = UCB(n_arms)\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5afdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: \n",
      " [-1.58 -1.95  0.9   2.04 -0.01 -1.01 -1.17  0.89  0.78 -1.67] \n",
      "\n",
      "Total Reward: 4006.51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function vars>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in range(T):\n",
    "    arm_to_pull = agent.select_arm()\n",
    "    reward = np.random.randn() + true_means[arm_to_pull]\n",
    "    agent.update(arm_to_pull, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Values: \\n {np.round(agent.values, 2)} \\n\")\n",
    "print(f\"Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "vars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
